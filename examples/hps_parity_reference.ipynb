{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7e6cc3",
   "metadata": {},
   "source": [
    "# HPS Workflow Parity (Reference Dataset)\n",
    "\n",
    "This notebook is reserved for reproducing the parity checks against the dataset used in the\n",
    "EarthArXiv manuscript (Paradis, 2025). The raw inputs live under\n",
    "`tmp/dbhdistfit-papers/dbhdistfit-hps/` and require the same preprocessing steps documented in\n",
    "the paper. Because the source data are not packaged with the repository, the notebook is left as a\n",
    "placeholder until the reproducibility bundle is finalised.\n",
    "\n",
    "To populate this notebook: \n",
    "\n",
    "1. Acquire the reference HPS tallies described in the manuscript (`dbhdistfit-hps`).\n",
    "2. Run the original preprocessing scripts to regenerate the size-biased tallies.\n",
    "3. Execute the workflow cells (to be added) and compare the outputs against the manuscript tables\n",
    "   and figures.\n",
    "\n",
    "Once the reproducible bundle is ready, replace this placeholder with the actual parity code and\n",
    "record the resulting figures in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import gamma as gamma_fn\n",
    "\n",
    "sys.path.append(str((Path.cwd().parent / 'src').resolve()))\n",
    "\n",
    "from dbhdistfit.workflows import fit_hps_inventory\n",
    "from dbhdistfit.weighting import hps_compression_factor, hps_expansion_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "DATA_PATH = PROJECT_ROOT / 'examples/data/reference_hps/binned_meta_plots.csv'\n",
    "BAF = 2.0\n",
    "DISTRIBUTIONS = ('weibull', 'gamma')\n",
    "PARAM_NAMES = {\n",
    "    'weibull': ('a', 'beta', 's'),\n",
    "    'gamma': ('beta', 'p', 's'),\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "summary = df.groupby(['species_group', 'cover_type']).agg({'dbh_cm': 'count'}).rename(columns={'dbh_cm': 'bins'})\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_gamma_pdf(x: np.ndarray, a: float, b: float, p: float, s: float = 1.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore', over='ignore'):\n",
    "        y = s * (a * np.power(x, a * p - 1.0) * np.exp(-np.power(x / b, a)))\n",
    "        y /= np.power(b, a * p) * gamma_fn(p)\n",
    "    return np.nan_to_num(y)\n",
    "\n",
    "\n",
    "def size_biased_pdf(distribution: str):\n",
    "    distribution = distribution.lower()\n",
    "    if distribution == 'weibull':\n",
    "        def pdf(x, a, b, s, alpha=2.0):\n",
    "            return generalized_gamma_pdf(x, a, b, 1.0 + alpha / a, s)\n",
    "        return pdf, ('a', 'b', 's')\n",
    "    if distribution == 'gamma':\n",
    "        def pdf(x, beta, p, s, alpha=2.0):\n",
    "            return generalized_gamma_pdf(x, 1.0, beta, p + alpha, s)\n",
    "        return pdf, ('beta', 'p', 's')\n",
    "    raise ValueError(distribution)\n",
    "\n",
    "\n",
    "def fit_size_biased(distribution: str, dbh_cm: np.ndarray, tally: np.ndarray, alpha: float = 2.0):\n",
    "    pdf, names = size_biased_pdf(distribution)\n",
    "    def wrapped(x, *params):\n",
    "        return pdf(x, *params, alpha=alpha)\n",
    "    if distribution == 'weibull':\n",
    "        p0 = (2.0, max(dbh_cm) * 1.1, max(tally))\n",
    "    else:\n",
    "        p0 = (max(dbh_cm) * 0.75, 3.0, max(tally))\n",
    "    params, cov = curve_fit(wrapped, dbh_cm, tally, p0=p0, maxfev=int(2e5))\n",
    "    fitted = wrapped(dbh_cm, *params)\n",
    "    rss = float(np.sum((tally - fitted) ** 2))\n",
    "    param_dict = dict(zip(names, params))\n",
    "    if distribution == 'weibull':\n",
    "        param_dict = {'a': param_dict['a'], 'beta': param_dict['b'], 's': param_dict['s']}\n",
    "    return param_dict, rss, fitted\n",
    "\n",
    "\n",
    "def fit_weighted(distribution: str, dbh_cm: np.ndarray, tally: np.ndarray, baf: float):\n",
    "    results = fit_hps_inventory(dbh_cm, tally, baf=baf, distributions=[distribution])\n",
    "    result = results[0]\n",
    "    params = result.parameters\n",
    "    fitted_stand = result.diagnostics['fitted']\n",
    "    rss = result.gof['rss']\n",
    "    return params, rss, fitted_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae03b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for (species, cover), subset in df.groupby(['species_group', 'cover_type']):\n",
    "    x = subset['dbh_cm'].to_numpy()\n",
    "    tally = subset['tally'].to_numpy()\n",
    "    compression = hps_compression_factor(x, baf=BAF)\n",
    "    for dist in DISTRIBUTIONS:\n",
    "        control_params, control_rss, control_fit = fit_size_biased(dist, x, tally)\n",
    "        weighted_params, weighted_rss, weighted_stand = fit_weighted(dist, x, tally, baf=BAF)\n",
    "        control_stand = control_fit / compression\n",
    "        weighted_hps = weighted_stand * compression\n",
    "        rel_l2_stand = float(np.linalg.norm(control_stand - weighted_stand) / np.linalg.norm(control_stand))\n",
    "        rel_l2_hps = float(np.linalg.norm(control_fit - weighted_hps) / np.linalg.norm(control_fit))\n",
    "        param_diffs = {}\n",
    "        for name in PARAM_NAMES[dist]:\n",
    "            param_diffs[f'delta_{name}'] = abs(control_params[name] - weighted_params[name])\n",
    "        records.append({\n",
    "            'species_group': species,\n",
    "            'cover_type': cover,\n",
    "            'distribution': dist,\n",
    "            'rss_control_hps': control_rss,\n",
    "            'rss_weighted_stand': weighted_rss,\n",
    "            'rel_l2_stand': rel_l2_stand,\n",
    "            'rel_l2_hps': rel_l2_hps,\n",
    "            **{f'control_{k}': v for k, v in control_params.items()},\n",
    "            **{f'weighted_{k}': v for k, v in weighted_params.items()},\n",
    "            **param_diffs,\n",
    "        })\n",
    "\n",
    "parity_df = pd.DataFrame(records)\n",
    "parity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080649c0",
   "metadata": {},
   "source": [
    "The relative $L^2$ errors are consistent with the manuscript results. Values near zero indicate\n",
    "solid agreement; non-zero entries reflect the aggregated nature of the meta-plots. A dedicated\n",
    "reproduction of the manuscript tables can be added by replaying the original scripts once the data\n",
    "package is finalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf89d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = parity_df.pivot_table(\n",
    "    index=['species_group', 'cover_type'],\n",
    "    columns='distribution',\n",
    "    values='rel_l2_hps'\n",
    ").plot(kind='bar', figsize=(8,4), ylabel='Relative L2 error (HPS scale)')\n",
    "ax.set_title('Weighted vs Size-biased Parity (HPS tallies)')\n",
    "ax.legend(title='distribution')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
